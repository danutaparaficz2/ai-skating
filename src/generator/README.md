# Generator Module - RAG mit Alibaba Cloud

Retrieval-Augmented Generation (RAG) System fÃ¼r Athleten-Informationen mit FAISS Vector Store und Alibaba Cloud Qwen API.

## ğŸ“‹ Ãœberblick

Dieses Modul ermÃ¶glicht:
- âœ… Semantische Suche in Athleten-Dokumenten (FAISS)
- âœ… Q&A Ã¼ber Athleten mit kontextbasierter Generierung
- âœ… Chat-Konversationen mit History
- âœ… Automatische Story-Generierung
- âœ… Integration mit Alibaba Cloud Qwen LLM

## ğŸ—ï¸ Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚
â”‚   Query     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAGGenerator       â”‚
â”‚  - Orchestriert     â”‚
â”‚    Pipeline         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                  â”‚
       â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retriever    â”‚   â”‚ LLM Client  â”‚
â”‚ - FAISS      â”‚   â”‚ - Qwen API  â”‚
â”‚ - MongoDB    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Komponenten

### 1. `config.py`
Zentrale Konfiguration fÃ¼r:
- FAISS Index Pfade
- MongoDB Verbindungen
- Qwen API Settings
- RAG Parameter (top_k, similarity)
- LLM Parameter (temperature, max_tokens)

### 2. `retriever.py`
**FAISSRetriever** - Semantische Suche
- LÃ¤dt FAISS-Index und Metadaten
- Query â†’ Embedding â†’ Ã„hnlichkeitssuche
- Filtert nach Athlet, Similarity-Threshold
- Gibt relevante Chunks mit Metadaten zurÃ¼ck

### 3. `llm_client.py`
**QwenClient** - Alibaba Cloud Integration
- OpenAI-kompatible API
- Chat Completion
- Text Generation
- Context-basierte Generation

### 4. `rag_generator.py`
**RAGGenerator** - Hauptklasse
- `generate()`: Einzelne Frage beantworten
- `chat()`: Konversation mit History
- `generate_story()`: Story-Generierung
- Orchestriert Retrieval + Generation

### 5. `cli.py`
Command-Line Interface
- `ask`: Frage stellen
- `story`: Story generieren
- `chat`: Interaktiver Chat

## ğŸš€ Installation

```bash
cd /Users/kaplank/Privat/FFHS/Deepl_Hackathon/ai-skating/src/generator
pip install -r requirements.txt
```

## âš™ï¸ Konfiguration

### API Key setzen

```bash
export QWEN_API_KEY="sk-your-alibaba-cloud-key"
```

### Weitere Optionen (optional)

```bash
# Pfade
export FAISS_INDEX_PATH="/path/to/faiss_indexes"
export MONGO_URI="mongodb://localhost:27017/"
export MONGO_DB="crawler"

# RAG Parameter
export TOP_K_CHUNKS=5
export MIN_SIMILARITY=0.3

# LLM Parameter
export QWEN_MODEL="qwen-plus"
export TEMPERATURE=0.7
export MAX_TOKENS=1000
```

## ğŸ’» Verwendung

### CLI

```bash
# Frage stellen
python -m cli ask "Was sind die Erfolge von Kristen Santos-Griswold?"

# Mit Athleten-Filter
python -m cli ask "Welche Medaillen hat sie gewonnen?" \
  --athlete "Kristen Santos-Griswold"

# Story generieren
python -m cli story "Kristen Santos-Griswold" \
  --type profile \
  --style engaging \
  -o story.txt

# Interaktiver Chat
python -m cli chat --athlete "Kristen Santos-Griswold"
```

### Python API

```python
from generator import RAGGenerator

# Initialisierung
generator = RAGGenerator()

# Frage stellen
result = generator.generate(
    query="Was sind die grÃ¶ÃŸten Erfolge?",
    athlete_name="Kristen Santos-Griswold"
)

print(result['answer'])
print(f"Quellen: {result['sources']}")

# Cleanup
generator.close()
```

Siehe [QUICKSTART.md](QUICKSTART.md) fÃ¼r mehr Beispiele.

## ğŸ“Š Response Format

```python
{
    "answer": "Generierte Antwort basierend auf Chunks...",
    "sources": [
        {
            "id": 1,
            "athlete": "Kristen Santos-Griswold",
            "similarity": 0.89,
            "url": "https://example.com",
            "topic": "achievements",
            "preview": "Text preview..."
        }
    ],
    "metadata": {
        "chunks_used": 5,
        "athlete_filter": "Kristen Santos-Griswold",
        "model": "qwen-plus",
        "top_similarity": 0.89
    }
}
```

## ğŸ¯ Use Cases

### 1. Q&A System
```python
result = generator.generate(
    query="Wie viele Olympia-Medaillen hat Kristen Santos-Griswold?",
    athlete_name="Kristen Santos-Griswold"
)
```

### 2. Story Generation
```python
story = generator.generate_story(
    athlete_name="Kristen Santos-Griswold",
    story_type="achievement",
    style="dramatic"
)
```

### 3. Chat Interface
```python
# Erste Frage
r1 = generator.chat(query="ErzÃ¤hl mir Ã¼ber Kristen")

# Follow-up mit Kontext
r2 = generator.chat(
    query="Was sind ihre grÃ¶ÃŸten Erfolge?",
    conversation_history=r1['conversation_history']
)
```

### 4. Multi-Athleten-Vergleich
```python
result = generator.generate(
    query="Vergleiche die Erfolge verschiedener Short-Track Athleten"
    # Kein athlete_name Filter â†’ sucht Ã¼ber alle
)
```

## ğŸ”§ Advanced Usage

### Custom Config

```python
from generator import GeneratorConfig, RAGGenerator

config = GeneratorConfig(
    # Mehr Kontext holen
    top_k_chunks=10,
    min_similarity=0.2,
    
    # LLM Settings
    qwen_model="qwen-max",  # Bestes Modell
    temperature=0.9,
    max_tokens=2000,
    
    # Custom System Prompt
    system_prompt="Du bist ein Experte fÃ¼r Short-Track Eisschnelllauf..."
)

generator = RAGGenerator(config=config)
```

### Nur Retrieval (ohne LLM)

```python
from generator import FAISSRetriever

retriever = FAISSRetriever(
    faiss_index_path="...",
    mongo_uri="mongodb://localhost:27017/",
    mongo_db="crawler",
    mongo_collection="athlete_chunks"
)

chunks = retriever.retrieve(
    query="Olympic medals",
    athlete_name="Kristen Santos-Griswold",
    top_k=10
)

for chunk in chunks:
    print(f"{chunk['similarity']:.2f}: {chunk['text'][:100]}")
```

## ğŸ§ª Testing

```bash
# Teste mit Beispielen
python examples.py

# CLI Test
python -m cli ask "Test question" --athlete "Kristen Santos-Griswold" -v
```

## ğŸ“ˆ Performance Optimierung

### 1. Top-K Parameter
- Weniger Chunks (top_k=3): Schneller, weniger Kontext
- Mehr Chunks (top_k=10): Langsamer, mehr Kontext

### 2. Similarity Threshold
- HÃ¶her (0.5): Nur sehr relevante Chunks
- Niedriger (0.2): Mehr Chunks, evtl. weniger relevant

### 3. Modell-Wahl
- `qwen-turbo`: Schnell, gÃ¼nstig
- `qwen-plus`: Ausgewogen (Default)
- `qwen-max`: Beste QualitÃ¤t, langsamer

### 4. Batch Processing
```python
# Mehrere Fragen zusammen verarbeiten
questions = [...]
for q in questions:
    result = generator.generate(q)
```

## ğŸ” Troubleshooting

### Problem: API Key Error
```
ValueError: QWEN_API_KEY muss gesetzt sein
```
**LÃ¶sung**: `export QWEN_API_KEY="your-key"`

### Problem: FAISS Index nicht gefunden
```
FileNotFoundError: FAISS-Index nicht gefunden
```
**LÃ¶sung**: FÃ¼hre zuerst Transformer aus
```bash
cd ../transformer
python -m cli index "Athlete Name"
```

### Problem: Keine Chunks gefunden
```
Leider habe ich keine relevanten Informationen gefunden
```
**LÃ¶sung**: 
- PrÃ¼fe ob Athlet in MongoDB existiert
- Verringere `min_similarity`
- ErhÃ¶he `top_k_chunks`

### Problem: Schlechte Antwort-QualitÃ¤t
**LÃ¶sung**:
- Verwende `qwen-max` statt `qwen-plus`
- ErhÃ¶he `top_k_chunks` fÃ¼r mehr Kontext
- Passe `system_prompt` an

## ğŸ“š Weitere Dokumentation

- [QUICKSTART.md](QUICKSTART.md) - Quick Start Guide
- [examples.py](examples.py) - Code-Beispiele
- [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md) - Implementation Details

## ğŸ”— Dependencies

- `sentence-transformers`: Embedding-Generierung
- `faiss-cpu`: Vector Store
- `pymongo`: MongoDB-Zugriff
- `requests`: Alibaba Cloud API
- `torch`: ML Backend

## ğŸ“ Lizenz

Internes Projekt - FFHS Deep Learning Hackathon

## ğŸ¤ Contributing

Siehe Hauptprojekt README fÃ¼r Details.

